{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S-8Tg9jcIq3"
   },
   "source": [
    "<h1> TDA course project </h1>\n",
    "\n",
    "Name: Botond Ortutay\n",
    "\n",
    "Pair: If you did this project as pair work, name the other student here, leave empty otherwise. If you work in pair, <b>both</b> hand out the same project report in Moodle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment & library info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming this is running in a python venv with all the necessary libraries pre-installed.\n",
    "\n",
    "Assuming the following file structure:\n",
    "\n",
    "```\n",
    ".\n",
    "‚îú‚îÄ‚îÄ project_template.ipynb\n",
    "‚îî‚îÄ‚îÄ tda25-responses.jsonl.gz\n",
    "```\n",
    "\n",
    "The data has been downloaded from http://dl.turkunlp.org/tda-course-2025/tda25-responses.jsonl.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "# Reading the data\n",
    "import gzip\n",
    "import jsonlines\n",
    "from typing import List, Dict\n",
    "\n",
    "# Data-exploration & preparation\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "# Raw data to Huggingface datasets\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tokenization\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# The model\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# Training the model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Evaluation\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for opening jsonl.gz\n",
    "Source: <br>\n",
    "serialize_jsolines_gz.py, https://gist.github.com/luminoso/0581b7f6760ea9a26b06115c2993f351 <br>\n",
    "by Guilherme Cardoso, https://github.com/luminoso <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl_gz(filename) -> List[Dict]:\n",
    "    data = []\n",
    "    with gzip.open(filename) as fp:\n",
    "        j_reader = jsonlines.Reader(fp)\n",
    "\n",
    "        for obj in j_reader:\n",
    "            data.append(obj)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6i74smlNdZyn"
   },
   "source": [
    "<h1> Step 1: Load the data with LLM judgements </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YGfc3GN5efOz"
   },
   "outputs": [],
   "source": [
    "myJsonlGz = read_jsonl_gz(\"tda25-responses.jsonl.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': 'Peeling an onion seems like an trivial task, but if you‚Äôve never '\n",
      "             'peeled an onion before, it can be quite intimidating. Don‚Äôt '\n",
      "             'worry ‚Äì it is pretty easy to peel an onion.\\n'\n",
      "             'You can now learn how to peel an onion by following these '\n",
      "             'illustrated step-by-step instructions.\\n'\n",
      "             'Step #1: Put the whole onion on the cutting board\\n'\n",
      "             'Step 2: Cut off one end of the onion with a knife, as shown on '\n",
      "             'the picture below:\\n'\n",
      "             'Here‚Äôs a picture of the onion with that end already cut off. The '\n",
      "             'end of the onion is laying on the right side of the onion on the '\n",
      "             'cutting board.\\n'\n",
      "             'Step 3: Cut off another end of the onion with a knife, as show '\n",
      "             'on the picture below:\\n'\n",
      "             'After both ends of the onion have been cut off, the onion is '\n",
      "             'ready to be peeled. Here‚Äôs the picture of the onion without its '\n",
      "             'ends:\\n'\n",
      "             'Step 4: Start peeling! Make a cut under the peel, and pull on '\n",
      "             'the peel so it separates from the onion. Look at the picture: '\n",
      "             'knife under the peel, thumb on top of the peel. Grab the peel '\n",
      "             'and pull.\\n'\n",
      "             'Step 5: Keep peeling in the same way as the previous step, until '\n",
      "             'there is no more peel left on the onion.\\n'\n",
      "             'If the peel doesn‚Äôt come off easily, you can just peel the outer '\n",
      "             'thin layer of onion itself, like shown picture below:\\n'\n",
      "             'You are done! The onion is now peeled and ready to be used in '\n",
      "             'your recipe!\\n'\n",
      "             'Here‚Äôs the picture of the fully peeled onion:',\n",
      " 'response': 'Step-by-step: Yes  \\n'\n",
      "             'Training: No  \\n'\n",
      "             '\\n'\n",
      "             'The article does provide a clear sequence of ordered steps for '\n",
      "             'peeling an onion, making it easy for readers to follow the '\n",
      "             'instructions. However, while the instructions are '\n",
      "             'straightforward, they lack the depth and detail needed for '\n",
      "             'training a language model capable of deeper reasoning, as they '\n",
      "             'do not include alternative methods, potential problems, or '\n",
      "             'reasoning behind each step that would enhance understanding.'}\n",
      "{'document': 'Cowboy‚Äôs WR Terrance Williams Arrested\\n'\n",
      "             \"Dallas Cowboy's wide receiver Terrance Williams was arrested \"\n",
      "             'early Saturday morning for public intoxication and leaving the '\n",
      "             'scene of an accident. Williams wrecked his Lamborghini at about '\n",
      "             '3am Saturday morning and proceeded to leave the scene, only to '\n",
      "             'be arrested 2 hours later. Williams released the following '\n",
      "             'statement.\\n'\n",
      "             '\"I am grateful that no one was injured in the accident. The '\n",
      "             'driver in front of me slammed on his brakes and I turned to the '\n",
      "             'left and hopped the curb to avoid hitting him. I got his '\n",
      "             'insurance information and my neighbor picked me up when my car '\n",
      "             \"wouldn't drive. I live right near where the accident occurred, \"\n",
      "             'so my neighbor dropped me off and I called a tow truck and took '\n",
      "             'the scooter from my house to go meet the tow truck driver. The '\n",
      "             'police officer, who I have met in the past in the neighborhood, '\n",
      "             'saw me on the scooter and arrested me without performing any '\n",
      "             'sobriety tests. I have always been an upstanding citizen and '\n",
      "             'handled the situation the best way I know how. I apologize if I '\n",
      "             'should have handled it a little bit differently.\"\\n'\n",
      "             'This statement does raise some questions, and not just about '\n",
      "             'Williams behavior, but the situation in general.\\n'\n",
      "             '- How fast was Williams going in order to not be able to brake '\n",
      "             'properly?\\n'\n",
      "             '- When did Williams drink in order to get the public '\n",
      "             'intoxication charge?\\n'\n",
      "             '- Was Williams intoxicated?\\n'\n",
      "             '- Williams owns a Lamborghini AND a scooter?\\n'\n",
      "             \"Lots of questions still to be asked about this, but it'll most \"\n",
      "             \"likely get swept under the rug, because he's in the NFL, and \"\n",
      "             \"he's a Dallas Cowboy.\",\n",
      " 'response': 'Step-by-step: No  \\n'\n",
      "             'Training: No  \\n'\n",
      "             '\\n'\n",
      "             'The article does not provide a clear sequence of ordered steps '\n",
      "             'to guide a reader through completing a specific task; rather, it '\n",
      "             'narrates an event involving Terrance Williams and raises '\n",
      "             'questions about the incident. Additionally, the instructions or '\n",
      "             'narrative are not structured in a way that would support '\n",
      "             'training a language model for deeper reasoning, as there are no '\n",
      "             'detailed procedural instructions or logical frameworks presented '\n",
      "             'that would encourage critical analysis or reasoning about the '\n",
      "             'content.'}\n",
      "{'document': 'Idea Crib‚Äôs Well-thy Pinoy Profiles blog series is now '\n",
      "             'underway!! With this blog series, I plan to regularly feature '\n",
      "             'healthy and happy Pinoys and glean insights about why and how '\n",
      "             'they keep healthy and happy. The ultimate aim is to inspire more '\n",
      "             'individuals to take care of themselves by taking care of their '\n",
      "             'health and their bodies.\\n'\n",
      "             'The first featured personality in this blog series is my friend '\n",
      "             'Carlo, a marathoner and Certified Investment Solicitor. He also '\n",
      "             'just finished his first triathlon last week. Check out his '\n",
      "             'answers in the blog‚Äôs latest post.\\n'\n",
      "             'Well-thy Pinoy Profiles: Get to Know Carlo Sicat\\n'\n",
      "             'Occupation Senior Financial Adviser & Certified Investment '\n",
      "             'Solicitor\\n'\n",
      "             'Hobbies Triathlon, Ultimate Frisbee, and Backpack Traveling\\n'\n",
      "             'Social media profile IG: @carlokohan\\n'\n",
      "             'How do you stay healthy?\\n'\n",
      "             'I stay healthy by making sure I do physical activities daily or '\n",
      "             'every other day. I also make sure that I eat right and in '\n",
      "             'moderation.\\n'\n",
      "             'Why do you keep healthy?\\n'\n",
      "             'It feels good to be healthy. It boosts your confidence and it '\n",
      "             'also gives you benefits like not being sick and being able to '\n",
      "             'function well in your job.\\n'\n",
      "             'When did you decide to live healthy?\\n'\n",
      "             'I used to work in the BPO Industry. As we all know, the nature '\n",
      "             'of BPO work is kind of exhausting due to shifting schedules; I '\n",
      "             'gained a lot of weight while working in a BPO company.\\n'\n",
      "             'I used to be a fan of fast food. I loved eating in buffet and '\n",
      "             'indulging in sweets. I thought that as long as I was earning '\n",
      "             'good, I can eat whatever I want. Until I frequently got sick. I '\n",
      "             'remember getting hospitalized 3x a year due to gastritis. I was '\n",
      "             'obese by then. I thought I was healthy because I looked healthy '\n",
      "             'but I wasn‚Äôt. That was when I decided to lose weight.\\n'\n",
      "             'I tried running thinking that was the fastest way to lose '\n",
      "             'weight. It proved effective for me. I learned to love it and it '\n",
      "             'eventually became part of my routine.\\n'\n",
      "             'What are your favorite indulgences/ cheat day treats?\\n'\n",
      "             'Ice cream. I can finish a 1.5 gallon in one seating. My favorite '\n",
      "             'flavor is cookies and cream. Buffets. I still can‚Äôt resist them. '\n",
      "             'Tenka at Glorietta 4 is my favorite so far.\\n'\n",
      "             'In your opinion, what‚Äôs the one thing people can do to '\n",
      "             'kick-start their journey to wellness?\\n'\n",
      "             'Did you know that eating healthy, doing physical activities, and '\n",
      "             'not smoking are the 3 easy behaviors we can do to not get sick? '\n",
      "             'According to studies, if we take these for granted, these may '\n",
      "             'lead to the top 4 chronic diseases like cardiovascular disease, '\n",
      "             'cancer, diabetes, and lung diseases which contribute to 50% of '\n",
      "             'deaths world wide. You don‚Äôt have to be an athlete or a gym rat. '\n",
      "             'Just make sure you stay active, eat healthy, and quit smoking '\n",
      "             '(if you‚Äôre a smoker, that is) because the only person who will '\n",
      "             'take care of the sick person you will be is the healthy you '\n",
      "             'now.\\n'\n",
      "             'I love that last line. Soo true. I also agree that you don‚Äôt '\n",
      "             'have to be an athlete to be healthy! What can you say about '\n",
      "             'Carlo‚Äôs journey to fitness and wellness? Who else can I feature '\n",
      "             'in this series? I‚Äôd love to hear your comments below.\\n'\n",
      "             'The next Well-thy Pinoy Profile is on March 30, 2016. Can you '\n",
      "             'guess who I‚Äôll feature? üôÇ Stay tuned!\\n'\n",
      "             'Join the mailing list:\\n'\n",
      "             'Like it? Share it!\\n'\n",
      "             'Photo from Carlo Sicat.\\n'\n",
      "             'Read my blog‚Äôs full disclosure and disclaimer here.',\n",
      " 'response': 'Step-by-step: No  \\n'\n",
      "             'Training: No  \\n'\n",
      "             '\\n'\n",
      "             'The article does not provide a clear sequence of ordered steps '\n",
      "             'for completing a task; instead, it features an informal '\n",
      "             'interview format where Carlo shares his personal experiences and '\n",
      "             'insights about health and wellness. Additionally, the '\n",
      "             'instructions are not structured in a manner that encourages '\n",
      "             'deeper reasoning or procedural understanding, as they lack a '\n",
      "             'systematic approach or detailed guidelines that would aid in '\n",
      "             'training a language model for complex reasoning tasks.'}\n",
      "{'document': 'Will I ever stop being nervous every time I publish a personal '\n",
      "             'blog post?\\n'\n",
      "             'My heart tells me it‚Äôs totally fine. Our blog readers have '\n",
      "             'always said they love reading about our lives & our travels & '\n",
      "             'our family, and I‚Äôve always told myself that when personal posts '\n",
      "             'stopped getting a response, I‚Äôd stop posting them. I enjoy '\n",
      "             'writing these posts as a record of how our family is growing, '\n",
      "             'and what‚Äôs going on in our lives. I think it keeps my writing '\n",
      "             'fresh & honest. We love that it helps new clients get to know us '\n",
      "             'better‚Ä¶ it so fun to meet people for the first time and hear '\n",
      "             'that they feel like they‚Äôve known us for years.\\n'\n",
      "             'But my brain? Oh, my brain freaks out when I write these posts. '\n",
      "             '‚ÄúWho are you kidding? No one CARES what you guys did at the '\n",
      "             'beach last week. Yeesh, how self-centered are you? You look '\n",
      "             'ridiculous doing this. Give up.‚Äù\\n'\n",
      "             'My brain is kind of a jerk sometimes, you‚Äôve noticed.\\n'\n",
      "             'As is the pattern my whole life through, my heart usually wins '\n",
      "             'out over my brain, and you get a post like the one today. '\n",
      "             'Nothing ground-breaking, no photographic amazingness to melt '\n",
      "             'your eyeballs‚Ä¶ just life. Summertime. Adventures with our '\n",
      "             'family. Memories we‚Äôre blessed to have made, and happy to '\n",
      "             'share.\\n'\n",
      "             'If you read this, thank you for siding with my heart. üôÇ\\n'\n",
      "             'We took the kids & my parents with us last week to Port Aransas, '\n",
      "             'and stayed once again at the Mayan Princess resort way at the '\n",
      "             'end of the island. We just love that property. The beach is '\n",
      "             'always so much more empty, and the condos are nice, and the '\n",
      "             'pools are wonderful. We typically don‚Äôt spend much time in the '\n",
      "             'pools at all, but if you‚Äôre friends with us on Facebook, you '\n",
      "             'read about my little encounter with a stingray. :/ I‚Äôm feeling '\n",
      "             'fine now, just a bit achy, but that definitely threw us off the '\n",
      "             'ocean for a day!\\n'\n",
      "             'Rather than saving my favorite images for last, I‚Äôm going to '\n",
      "             'lead with them for this post. I love that we have been coming to '\n",
      "             'Port Aransas for so many years in a row, and our kids know the '\n",
      "             'town and love it. I cherish these images of them we capture '\n",
      "             'every year, on the same beach, showing how much they‚Äôve grown. '\n",
      "             'Our beautiful children‚Ä¶\\n'\n",
      "             'We went on a Dolphin Watching tour one morning, out of Woody‚Äôs. '\n",
      "             'It was SO fun. We got to see lots of dolphins (although my '\n",
      "             'pictures of them aren‚Äôt that wonderful), and the boat was really '\n",
      "             'big & nice.\\n'\n",
      "             'As part of the tour that Jack REALLY loved, our boat got '\n",
      "             'attacked by pirates! üôÇ They shot us with water guns and came '\n",
      "             'aboard to sword fight with some of the passengers. It was such a '\n",
      "             'blast!\\n'\n",
      "             'Dorothy wasn‚Äôt such a fan.\\n'\n",
      "             'I think my favorite part of the morning was when the first mate '\n",
      "             '(a one eyed, scarred up old sailor with stories about shark '\n",
      "             'attacks) pulled all sorts of sea creatures up in a net and let '\n",
      "             'the kids hold the safe ones!\\n'\n",
      "             'It was just a short walk from our room at the Princess down to '\n",
      "             'the beach, which is nice when you‚Äôre the kind of people that '\n",
      "             'take a lot of stuff to the beach. üôÇ\\n'\n",
      "             'My dad bought a really cool kite, and it brought back so many '\n",
      "             'memories of him flying trick kites when I was a little kid!\\n'\n",
      "             'We didn‚Äôt get any pictures of the stingray that got me, nor did '\n",
      "             'we photograph my foot when it was huge & purple & bleeding‚Ä¶ but '\n",
      "             'we got a picture of the crab that stabbed Mack, so that‚Äôll have '\n",
      "             'to do. üôÇ\\n'\n",
      "             'On Friday before we made the 6.5 hour trek home, we went to the '\n",
      "             'USS Lexington in Corpus Christi. My dad is a WWII buff, so he‚Äôs '\n",
      "             'been wanting to go for years. We all really enjoyed the tours!\\n'\n",
      "             'There‚Äôs no such thing as an attractive group picture on the '\n",
      "             'beach. Someone is always squinting. No one‚Äôs hair is done. '\n",
      "             'WHATEVER. These photos of our family together are priceless. '\n",
      "             'We‚Äôre so grateful for such a great relationship with my folks, '\n",
      "             'and fun travel times together!\\n'\n",
      "             'It‚Äôs hard to believe that school will start for Jack in less '\n",
      "             'than two weeks, and that after our upcoming wedding in the '\n",
      "             'Dominican Republic and a quick trip to Vegas, the fall wedding '\n",
      "             'season is going to kick off with a bang. What a wonderful summer '\n",
      "             'it‚Äôs been!',\n",
      " 'response': 'Step-by-step: No  \\n'\n",
      "             'Training: No  \\n'\n",
      "             '\\n'\n",
      "             'The article does not provide a clear sequence of ordered steps '\n",
      "             'to complete a specific task, nor does it present structured '\n",
      "             'procedural instructions. Instead, it primarily offers a personal '\n",
      "             'narrative about family experiences and reflections on blogging, '\n",
      "             'lacking the detailed, organized format that would support deeper '\n",
      "             'reasoning or model training. The content is informal and '\n",
      "             'anecdotal, focusing on feelings and memories rather than on '\n",
      "             'systematic instruction.'}\n",
      "{'document': 'In today‚Äôs NHL rumors rundown, Toronto Maple Leafs GM Kyle Dubas '\n",
      "             'clarifies why he didn‚Äôt add a defenseman at this year‚Äôs NHL '\n",
      "             'Trade Deadline, the Winnipeg Jets might be a big spender in free '\n",
      "             'agency and defenseman Drew Doughty of the Los Angeles Kings '\n",
      "             'talks his struggles with the team‚Äôs current rebuild. Finally, '\n",
      "             'what is the NHL doing about the coronavirus concerns affecting '\n",
      "             'sports, especially as the virus enters North America?\\n'\n",
      "             'Maple Leafs Didn‚Äôt See a Long-Term Fix\\n'\n",
      "             'According to an article on TSN, Toronto Maple Leafs general '\n",
      "             'manager Kyle Dubas told The Athletic‚Äôs Pierre LeBrun the reason '\n",
      "             'he didn‚Äôt do anything to his blue line at this year‚Äôs trade '\n",
      "             'deadline was because he couldn‚Äôt find a long-term fix that would '\n",
      "             'solve the team‚Äôs problems.\\n'\n",
      "             'Dubas wasn‚Äôt interested in rentals or short-term fixes when he '\n",
      "             'believes their issue on defense is not a short-term problem. '\n",
      "             'Dubas said he received calls about defenseman Tyson Barrie, but '\n",
      "             'didn‚Äôt get what he needed back as part of the return. As a '\n",
      "             'result, he didn‚Äôt move him and has risked losing him for nothing '\n",
      "             'in free agency.\\n'\n",
      "             'Of course, many will argue Dubas created his own problems in '\n",
      "             'this area by signing so many expensive contracts at forward and '\n",
      "             'that he‚Äôll try to fix those problems by spending more over the '\n",
      "             'summer as the salary cap increases.\\n'\n",
      "             'Related: Top 3 All-Time Capitals Goalies\\n'\n",
      "             'Jets to Spend Money In Free Agency?\\n'\n",
      "             'Speaking of cap increases, Mike McIntyre of the Winnipeg Free '\n",
      "             'Press writes that the projected salary cap increase for next '\n",
      "             'year will entice teams to spend a little more and one of those '\n",
      "             'teams could be the Winnipeg Jets.\\n'\n",
      "             'Jets GM Kevin Cheveldayoff to go shopping with an extra $4-$6 '\n",
      "             'million to spend and with the termination of Dustin Byfuglien‚Äôs '\n",
      "             'contract likely during the summer, the Jets could have as much '\n",
      "             'as $22 million in available funds. Obviously, the Jets have some '\n",
      "             'internal housekeeping items to look after, such as re-signing '\n",
      "             'Jack Roslovic, Mason Appleton, Jansen Harkins, and Sami Niku.\\n'\n",
      "             'Related: Winnipeg Jets Jersey History\\n'\n",
      "             'Doughty Struggling With Kings Re-Build\\n'\n",
      "             '‚ÄúIt‚Äôs very hard, but it‚Äôs the position I‚Äôm in, and I gotta try '\n",
      "             'to stay positive every day, as hard as it is,‚Äù Drew Doughty told '\n",
      "             'TSN when asked about the Los Angeles Kings current rebuild, He '\n",
      "             'likes the prospects in the pipeline, but Doughty says it‚Äôs often '\n",
      "             'a struggle knowing where they were as a team not just a few '\n",
      "             'short years ago.\\n'\n",
      "             '‚ÄúI have to make the best of it and just try to get better every '\n",
      "             'day, both as a team and individually,‚Äù he added.\\n'\n",
      "             'Coronavirus Update for the NHL\\n'\n",
      "             'There are many in the NHL starting to think the coronavirus '\n",
      "             'issue is going to be a major issue that affects games, meetings '\n",
      "             'and other events related to North American sports, including the '\n",
      "             'NHL. So far, 36 events worldwide have been affected.\\n'\n",
      "             'Health officials had actually already requested that the San '\n",
      "             'Jose Sharks cancel Thursday‚Äôs game and Sharks Sports and '\n",
      "             'Entertainment issued a statement in which it said it was ‚Äúaware '\n",
      "             'of the recommendations‚Äù made by the county, but that Thursday '\n",
      "             'night‚Äôs game against the Minnesota Wild would ‚Äúgo on as '\n",
      "             'scheduled.‚Äù\\n'\n",
      "             'Commissioner Gary Bettman said the NHL has barred League '\n",
      "             'employees from overseas business travel and advised the media '\n",
      "             'that those who make personal trips to affected countries must be '\n",
      "             'quarantined out of the office for two weeks upon their return to '\n",
      "             'see if symptoms arise.\\n'\n",
      "             'Right now, the virus hasn‚Äôt hit the games yet, but the NHL is '\n",
      "             'keeping an extremely close eye on things.\\n'\n",
      "             'Catch up on all the latest NHL Rumors',\n",
      " 'response': 'Step-by-step: No  \\n'\n",
      "             'Training: No  \\n'\n",
      "             '\\n'\n",
      "             'The article does not provide a clear sequence of ordered steps '\n",
      "             'for completing a task, as it primarily discusses NHL rumors and '\n",
      "             'updates rather than offering instructions or a process to '\n",
      "             'follow. Additionally, the content lacks well-structured '\n",
      "             'procedural instructions that could enhance the reasoning '\n",
      "             'capabilities of a language model, as it primarily reports on '\n",
      "             'events and opinions rather than detailing methods or strategies '\n",
      "             'that would require deeper reasoning.'}\n",
      "{'document': 'thank you, Sally Cronin, for such a nice writeup in Smorgasbord '\n",
      "             'Magazine Blog today. I am honored to be part of The Sunday '\n",
      "             'Interview series. I‚Äôll be sharing this article on my Social '\n",
      "             'Media and my 2 blogs today, too. The magazine is just gorgeous! '\n",
      "             'And, I just read this fact:\\n'\n",
      "             '‚Äú76 thoughts on ‚ÄúSmorgasbord Blog Magazine ‚Äì The Sunday '\n",
      "             'Interview ‚Äì Getting to Know Lynda McKinney Lambert‚Äù\\n'\n",
      "             'My guest today is American poet and author Lynda Lambert who '\n",
      "             'shares what is in her briefcase and purse, her fashion sense, a '\n",
      "             'book close to her heart and dreams.\\n'\n",
      "             'About Lynda Lambert\\n'\n",
      "             'Lynda Lambert (b.1943) was born in Ellwood City,PA Her academic '\n",
      "             'training is in Fine Arts and Literature. She was a professor of '\n",
      "             'Fine Arts and Humanities until 2007 when her teaching career was '\n",
      "             'cut short suddenly due to sight loss. Her art has been exhibited '\n",
      "             'world wide since 1976.\\n'\n",
      "             'Her first book _Concerti: Psalms for the Pilgrimage_ is a '\n",
      "             'collection of essays, poems, and drawings she did over a 10 year '\n",
      "             'period while teaching in Salzburg, Austria\\n'\n",
      "             'Lynda McKinney Lambert lost much of her sight in the fall of '\n",
      "             '2007, cue to Ischemic Optic Neuropathy. She retired from '\n",
      "             'teaching full-time a year later.\\n'\n",
      "             'She writes and makes mixed-media fiber art full-time. She uses '\n",
      "             'adaptive technologies for the‚Ä¶\\n'\n",
      "             'View original post 1,231 more words',\n",
      " 'response': 'Step-by-step: No  \\n'\n",
      "             'Training: No  \\n'\n",
      "             '\\n'\n",
      "             'The article does not provide a clear sequence of ordered steps '\n",
      "             'for completing a task; instead, it is more of an interview or '\n",
      "             'biographical piece that shares information about Lynda Lambert. '\n",
      "             'Additionally, it lacks well-structured procedural instructions '\n",
      "             \"that would facilitate training a language model, as it doesn't \"\n",
      "             'present detailed, systematic steps that encourage deeper '\n",
      "             'reasoning. Instead, it focuses on narrative content rather than '\n",
      "             'actionable guidance.'}\n",
      "{'document': 'Virtual Prayer Room 10.25.2020\\n'\n",
      "             'October 25 @ 11:00 am - 11:30 am\\n'\n",
      "             'Our amazing Prayer Practitioners are offering free, private '\n",
      "             'One-Minute Miracles after Sunday service using Zoom technology.\\n'\n",
      "             'If you desire a One-Minute Miracle, please follow these '\n",
      "             'instructions:\\n'\n",
      "             '- Join the Zoom meeting room after Sunday service (Zoom Link '\n",
      "             'Below). Miracles will be available from 11:00 AM to 11:30 AM '\n",
      "             'PST.\\n'\n",
      "             '- Once you are in the Zoom meeting room, you will be partnered '\n",
      "             'up with a Prayer Practitioner in a private discussion room.\\n'\n",
      "             'To Join, please click the ZOOM link and enter the meeting ID and '\n",
      "             'Password.\\n'\n",
      "             'Meeting ID: 837 9852 3499',\n",
      " 'response': 'Step-by-step: Yes  \\n'\n",
      "             'Training: No  \\n'\n",
      "             '\\n'\n",
      "             'The article does provide a clear sequence of ordered steps that '\n",
      "             'guide the reader on how to participate in the One-Minute '\n",
      "             'Miracles, indicating when to join the Zoom session and what to '\n",
      "             'expect upon entering. However, the instructions lack the depth '\n",
      "             'and detail needed to facilitate training a language model for '\n",
      "             'deeper reasoning; they do not elaborate on the purpose, context, '\n",
      "             'or elaborate variations of the process, which would be essential '\n",
      "             'for developing a nuanced understanding.'}\n",
      "{'document': 'Software & Apps File Types What Is an OVA File? How to open, '\n",
      "             'edit, & convert OVA files by Tim Fisher General Manager, VP, '\n",
      "             \"Lifewire.com Tim Fisher has 30+ years' professional technology \"\n",
      "             'support experience. He writes troubleshooting content and is the '\n",
      "             'General Manager of Lifewire. our editorial process Facebook '\n",
      "             'Twitter LinkedIn Tim Fisher Updated on May 13, 2020 reviewed by '\n",
      "             'Ryan Perian Lifewire Tech Review Board Member Ryan Perian is a '\n",
      "             'certified IT specialist who holds numerous IT certifications and '\n",
      "             \"has 12+ years' experience working in the IT industry support and \"\n",
      "             'management positions. our review board Article reviewed on Jul '\n",
      "             '15, 2020 Ryan Perian File Types Design Cryptocurrency MS Office '\n",
      "             'Windows Linux Google Drive Apps File Types Backup & Utilities '\n",
      "             'View More Tweet Share Email A file with the .OVA file extension '\n",
      "             'is most likely an Open Virtual Appliance file, sometimes called '\n",
      "             'an Open Virtual Application file or an Open Virtualization '\n",
      "             \"Format Archive file. They're used by virtualization programs to \"\n",
      "             'store various files associated with a virtual machine (VM). An '\n",
      "             'Open Virtual Appliance file is stored in the Open Virtualization '\n",
      "             'Format (OVF) as a TAR archive. Some of the files you might find '\n",
      "             'within it include disk images (like VMDKs), an OVF descriptor '\n",
      "             'XML-based text file, ISOs or other resource files, certificate '\n",
      "             'files, and an MF manifest file. Since the OVF format is a '\n",
      "             'standard, it can be used by a virtual machine program to export '\n",
      "             'the VM data files so that it can be imported into a different '\n",
      "             'application. VirtualBox, for example, can export one of its VMs '\n",
      "             'to an archive package with the .OVA file extension that includes '\n",
      "             'an OVF and VMDK file. OVA Files. Octava Musical Score files use '\n",
      "             'the OVA file extension, too, for musical scores created with the '\n",
      "             'Octava program. Score formatting options like bars, staffs, and '\n",
      "             'notes are stored in the OVA file. Other technology terms use the '\n",
      "             'OVA abbreviation, too, but none of them have anything to do with '\n",
      "             'the file formats on this page. Some examples include Outlook '\n",
      "             'Voice Access, Original Video Animation, and Office Validation '\n",
      "             'Assistant. How to Open an OVA File VMware Workstation and '\n",
      "             'VirtualBox are two virtualization applications that can open OVA '\n",
      "             'files. Some other similar programs that support OVF include '\n",
      "             \"VMware's OVF Tool, IBM SmartCloud, Microsoft System Center \"\n",
      "             'Virtual Machine Manager, and Amazon Elastic Compute Cloud '\n",
      "             '(Amazon EC2). Since OVA files are archives that hold other data, '\n",
      "             'you can extract the contents out or browse through them with a '\n",
      "             'file unzip program like 7-Zip. Octava opens OVA files that are '\n",
      "             \"Octava Musical Score files. How to Convert OVA Files There's \"\n",
      "             'little reason to convert an actual OVA file but several reasons '\n",
      "             'why you might want to convert one or more files from inside the '\n",
      "             \"OVA archive. Keep that in mind when you're deciding what format \"\n",
      "             'you want the virtual machine to end up as. For example, you '\n",
      "             \"don't need to convert an OVA file to OVF or VMDK in order to get \"\n",
      "             'those files out of the archive. You can instead just extract it '\n",
      "             'from the OVA file using one of the file unzip programs mentioned '\n",
      "             'above. The same is true if you want to convert the VMDK file to '\n",
      "             \"Hyper-V VHD; you can't just convert the OVA archive to VHD. \"\n",
      "             'Instead, you need to pull the VMDK file out of the OVA file and '\n",
      "             'then convert the VMDK to VHD using a program like Microsoft '\n",
      "             'Virtual Machine Converter. To convert an OVA file to be used '\n",
      "             'with VMware Workstation is as easy as exporting the VM to an OVA '\n",
      "             'file. Then, in VMware, use the File > Open menu to browse for '\n",
      "             'the OVA file, and then follow the instructions in VMware '\n",
      "             \"Workstation to set up the new VM. If the VM program you're using \"\n",
      "             \"doesn't export to an OVA file, VMware can still open other VM \"\n",
      "             'related content like OVF files. QCOW2 files are QEMU Copy On '\n",
      "             'Write Version 2 Disk Image files that are similar to other '\n",
      "             'virtual machine hard drive files. See this tutorial at Edoceo to '\n",
      "             'learn how to convert the OVA file to QCOW2 for use with QEMU. '\n",
      "             \"You might also be looking for an OVA to ISO converter but it'd \"\n",
      "             'be more appropriate to convert the virtual hard drive files '\n",
      "             '(that are inside the OVA archive) to an image format (much like '\n",
      "             'the VHD example above), which is out of the scope of this '\n",
      "             'article. VMware OVF Tool is a command-line tool that lets you '\n",
      "             'import and export OVA files to and from other VMware products. '\n",
      "             \"VMware vCenter Converter works, too. Still Can't Open the File? \"\n",
      "             \"If your file isn't opening with the suggestions above, \"\n",
      "             \"double-check that you're actually dealing with a file that ends \"\n",
      "             'with \".OVA\". This isn\\'t always the case since it\\'s easy to '\n",
      "             'confuse file formats that use similarly spelled file extensions. '\n",
      "             'For example, OVR and OVP are both spelled almost exactly like '\n",
      "             'OVA but are instead Overlay files used with a program called The '\n",
      "             'Overlay Maker. Trying to open either file format with the '\n",
      "             'virtualization tools mentioned above will not get you anywhere. '\n",
      "             'Similar to Octava Musical Score files are Overture Musical Score '\n",
      "             \"files that use the OVE file extension. It'd be easy to confuse \"\n",
      "             'these two file formats but the latter only works with the '\n",
      "             'Overture application.',\n",
      " 'response': '**Step-by-step: No**  \\n'\n",
      "             '**Training: No**  \\n'\n",
      "             \"The article doesn't provide a clear sequence of ordered steps \"\n",
      "             'intended for completing a task in a straightforward manner, '\n",
      "             'which is essential for a step-by-step guide. While it offers '\n",
      "             'some procedural information related to handling OVA files, the '\n",
      "             'lack of structured, detailed, and sequential instructions limits '\n",
      "             'its utility for training a language model capable of deeper '\n",
      "             'reasoning. The information presented is informative but more '\n",
      "             'descriptive than prescriptive, which would not effectively '\n",
      "             'facilitate reasoning or learning for specific tasks.'}\n",
      "{'document': 'We are on a mission to make better-for-you treats accessible to '\n",
      "             'everyone by challenging the status quo, provoke bold taste and '\n",
      "             'finally move Free-From, plant based and natural snacks out of '\n",
      "             'the counter-culture and into the mainstream way of eating. Our '\n",
      "             'snack range will leave you feeling nutritionally upgraded '\n",
      "             'without trading anything out. Especially not taste!\\n'\n",
      "             'Mimselicious snacks are always naturally gluten-, dairy- and '\n",
      "             'refined sugar-free. In fact they are produced in small batches '\n",
      "             'by a family run business in the UK, which allows us to use the '\n",
      "             'best quality ingredients so that our treats taste delicious '\n",
      "             'without all the nasty preservatives and additives.\\n'\n",
      "             'Working closely with our manufacturer allows us to implement a '\n",
      "             'just-in-time production that reduces food waste and gives you '\n",
      "             'the freshest ingredients on shelf.\\n'\n",
      "             'Our aim is to bring you mind-blowing indulgence at your '\n",
      "             'fingertips. No fad diets or fancy superfoods. Just bold taste, '\n",
      "             'natural ingredients without the preaching! Because everyone '\n",
      "             'should treat themselves once in a while!\\n'\n",
      "             'Welcome to MIMSELICIOUS!\\n'\n",
      "             'My name is Mirjam, the founder, and creator of Mimselicious. I '\n",
      "             'initially started the blog back in 2013 after discovering how my '\n",
      "             'food intolerances were affecting my overall wellbeing from '\n",
      "             'digestion problems to low energy.\\n'\n",
      "             'The discovery challenged my entire approach to cooking and '\n",
      "             'eating. I had to reinvent my pantry using more wholesome '\n",
      "             'ingredients to satisfy my sweet tooth whilst still maintaining '\n",
      "             'the excitement to make and eat it. Growing up spoilt with '\n",
      "             'delicious home-cooked foods and cuisine from across the globe it '\n",
      "             'was my aim to never compromise on taste and destroy the myth '\n",
      "             'that gluten-, sugar- or dairy free had to be bland and boring.\\n'\n",
      "             'A blog was born for friends and family‚Äôs curiosity and along '\n",
      "             'with it, catering stints emerged in the process for the likes of '\n",
      "             'Sam Smith, Stella McCartney, Tabitha Simmonds and Karla Otto. '\n",
      "             'Occasionally it even took me abroad for in-house catering jobs '\n",
      "             'for private parties and cooking classes.\\n'\n",
      "             'In 2017 I then decided there wasn‚Äôt anything on the market that '\n",
      "             'truly made my taste buds jump up with excitement and with '\n",
      "             'friends and family pushing me to do my own thing I looked into '\n",
      "             'professionally manufacturing my most popular sweet recipe from '\n",
      "             'the blog. Shortly after I quit my job and twenty months later, '\n",
      "             'with a lot of highs and lows a healthier twist on some famous '\n",
      "             'sweet treats were born.\\n'\n",
      "             'It took multiple setbacks with manufacturers making it clear '\n",
      "             'that the product was too complicated to produce or demanding '\n",
      "             'minimum order quantities that were not feasible for a small '\n",
      "             'start-up like ours. Yet after endless research, we finally found '\n",
      "             'a family run company that embraced all of the challenges put in '\n",
      "             'front of them as well as the ethos of Mimselicious. With the '\n",
      "             'right manufacturer in place, it only took us a couple of weeks '\n",
      "             'to perfect the recipes that were created over six years ago for '\n",
      "             'a larger scale production.\\n'\n",
      "             'Finally, this May 2019, we will be launching our very first '\n",
      "             'product line. We are sure you will love them just as much as we '\n",
      "             'do after taste-testing them for over six months! So, keep your '\n",
      "             'eyes peeled for the official launch date along with exciting '\n",
      "             'competitions and updates on future product launches.\\n'\n",
      "             'We love hearing from you guys, so please do get in touch if you '\n",
      "             'have any questions or comments!',\n",
      " 'response': 'Step-by-step: No  \\n'\n",
      "             'Training: No  \\n'\n",
      "             '\\n'\n",
      "             'The article does not provide a clear sequence of ordered steps '\n",
      "             'for completing a task, as it primarily shares a narrative about '\n",
      "             \"the author's journey and the philosophy behind their snack \"\n",
      "             'brand, rather than structured instructions. Additionally, the '\n",
      "             'content lacks detailed procedural instructions that would aid in '\n",
      "             'training a language model for deeper reasoning, as it does not '\n",
      "             'articulate specific methods or processes that could be broken '\n",
      "             'down into actionable steps.'}\n",
      "{'document': 'Karen Ferguson discovers a book with plenty of practical advice '\n",
      "             'for new SENCOs, and a healthy respect fo workload and wellbeing\\n'\n",
      "             'Any aspiring or newly appointed primary SENCO will benefit from '\n",
      "             'the wealth of experience Jackie Ward has drawn on to write this '\n",
      "             'book. It‚Äôs an easy-to-read and realistic ‚Äúhow to‚Äù guide that '\n",
      "             'shows a real appreciation for the reality of the job on the '\n",
      "             'ground. With teacher workload and wellbeing causing so much '\n",
      "             'concern across the education system, it‚Äôs a testament to Ward '\n",
      "             'that this theme is the golden thread that ties all the chapters '\n",
      "             'together.\\n'\n",
      "             'The book itself seems designed with this in mind. The layout is '\n",
      "             'clear and accessible, drawing attention to the interesting case '\n",
      "             'studies, effective top tips and key ‚Äútake aways‚Äù from each of '\n",
      "             'its 11 chapters.\\n'\n",
      "             'For a more experienced practitioner, some of the advice could '\n",
      "             'possibly border on patronising. However, there are enough golden '\n",
      "             'nuggets throughout to make it a good read for even the most '\n",
      "             'seasoned SENCO, and in any case, positive reinforcement is '\n",
      "             'always comforting in what can be an isolating role.\\n'\n",
      "             'Each chapter focuses on a specific aspect of a SENCO‚Äôs '\n",
      "             'responsibilities, and Ward‚Äôs singular focus throughout is to '\n",
      "             'give practical advice to move readers forward in their '\n",
      "             'development of the role. Early chapters break down the support '\n",
      "             'that SENCOs can give to children, parents and colleagues, with '\n",
      "             'each group of stakeholders getting their own dedicated chapter. '\n",
      "             'The chapter for children is linked directly to current '\n",
      "             'guidelines, the one on parental support is a good starting point '\n",
      "             'for signposting and meetings, and the one on working with '\n",
      "             'colleagues (in reality, often the most challenging part of the '\n",
      "             'job) clearly identifies their most common areas of need and '\n",
      "             'provides useful advice on how to support them.\\n'\n",
      "             'It is great to see wellbeing and workload being discussed\\n'\n",
      "             'A later chapter on working with outside agencies will be very '\n",
      "             'beneficial for any new SENCO. It sets out clearly what each '\n",
      "             'agency‚Äôs role is in supporting students, and the ‚ÄúPros and Cons‚Äù '\n",
      "             'section is of particular interest. Knowing that the challenges '\n",
      "             'of waiting lists and appointments are not just in your own area '\n",
      "             'is comforting in its own right.\\n'\n",
      "             'For me, the most interesting and useful chapter is the one '\n",
      "             'entitled ‚ÄúApplying for statutory assessment: EHCPs‚Äù ‚Äì a '\n",
      "             'fantastic go-to guide with step-by-step advice on the process. '\n",
      "             'Starting with who may request, and providing what is almost a '\n",
      "             'checklist of what is needed before even getting started, it goes '\n",
      "             'on to lay out a clear timeline of the process. The latter is '\n",
      "             'clear about what to expect at each stage, how to use the EHCP '\n",
      "             'once it is in place, and even how to handle appeals and '\n",
      "             'mediation ‚Äì an area that is not often openly discussed or '\n",
      "             'shared. This will be invaluable for anyone working with EHCPs '\n",
      "             'for the first time. I certainly would have appreciated it when I '\n",
      "             'started.\\n'\n",
      "             'Unfortunately, for all the focus on workload and wellbeing, the '\n",
      "             'advice on how to manage it is not as strong or as practical as '\n",
      "             'the advice on doing the job itself. While the book will no doubt '\n",
      "             'help new SENCOs hack their way to more efficient practices, most '\n",
      "             'of the specific tips for workload and wellbeing appear to rely '\n",
      "             'on asking senior leaders for dispensation. In an ideal world, '\n",
      "             'this should be sufficient but, especially in smaller and more '\n",
      "             'rural schools, SENCOs not only have teaching commitments but '\n",
      "             'other responsibilities too. Having said that, it is great to see '\n",
      "             'wellbeing and workload being discussed, and the emphasis on the '\n",
      "             'need to look after oneself before you can support others is '\n",
      "             'something all new practitioners should be taught.\\n'\n",
      "             'Overall then, this is a very welcome book that can only be a '\n",
      "             'benefit to newly appointed or aspiring SENCOs. With guidance '\n",
      "             'linked to the latest legislation, and a clear structure taking '\n",
      "             'in all the role‚Äôs main responsibilities, Ward has given us an '\n",
      "             'excellent tool to ensure more can get on the right track towards '\n",
      "             'sustained and sustainable careers in primary SEND.',\n",
      " 'response': 'Step-by-step: Yes  \\n'\n",
      "             'Training: Yes  \\n'\n",
      "             '\\n'\n",
      "             'The article clearly outlines a sequence of steps in the context '\n",
      "             'of applying for statutory assessment, specifically regarding '\n",
      "             'EHCPs (Education, Health and Care Plans). This structured '\n",
      "             'approach, which includes a checklist and a timeline of the '\n",
      "             'process, provides sufficient detail for readers to follow along '\n",
      "             'and deepen their understanding. Additionally, the procedural '\n",
      "             'nature of this guidance is conducive to training a language '\n",
      "             'model for reasoning, as it highlights systematic problem-solving '\n",
      "             'and the various stages involved in the process.'}\n"
     ]
    }
   ],
   "source": [
    "# Just printing a few elements to know what we're dealing with...\n",
    "for i in range(10):\n",
    "    pprint.pprint(myJsonlGz[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see each dataitem has a \"document\" and a \"response\" field. As defined in the instructions the response consists of 3 things: an AI-evaluation of whether the document is step-by-step data, an AI evaluation whether the document is reasoning data, and a short summary of the document. The format could be defined as:\n",
    "\n",
    "```\n",
    "Step-by-step: Yes/No\n",
    "Training: Yes/No\n",
    "\n",
    "Summary\n",
    "```\n",
    "\n",
    "We want to extract the AI-evaluations into their own datafield for classifier training later. This could be done by a simple regex search because of a well-defined format. Except there is a problem: the LLM sometimes decides to insert extra space and \\*-characters into the response for no reason. Therefore I also wrote a regex thing to remove these extra characters. Now we should be able to extract the relevant informations into their own datafields and then turn this list-of-dictionaries-thing into a proper Huggingface dataset. But before we do that. I looped through all the responses in the data and found one single item where the format is not followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': 'All our after-sale service staff is professional and patience so '\n",
      "             \"you don't need to have any worry anything about purchasing our \"\n",
      "             'IBM C1000-141 exam simulation: IBM Maximo Manage v8.x '\n",
      "             'Administrator, The C1000-141 pdf training guide can help you to '\n",
      "             'figure out the actual area where you are confused, IBM C1000-141 '\n",
      "             'Latest Study Materials As the unprecedented intensity of talents '\n",
      "             'comes in great numbers, what abilities should a talent of modern '\n",
      "             'time possess and finally walk to the success, So with our '\n",
      "             'C1000-141 exam questions, not only you can pass the exam with '\n",
      "             'ease with 100% pass guarantee, but also you can learn the most '\n",
      "             'professional and specilized knowledge in this field!\\n'\n",
      "             'With the help of our learning materials, especially the online '\n",
      "             'practice Latest C1000-141 Exam Book exam, you can practice IBM '\n",
      "             'Maximo Manage v8.x Administrator test questions in the formal '\n",
      "             'test environment and test your skills regarding IBM Maximo '\n",
      "             'Manage v8.x Administrator pass guaranteed.\\n'\n",
      "             'The Launcher can be customized easily to include applications, '\n",
      "             'C1000-141 Latest Study Materials file folders, and more, '\n",
      "             \"according to the user's needs, A Data Science Methods, It sounds \"\n",
      "             \"trivial, but when you're buying all of your shipping products, \"\n",
      "             'and spending hundreds C1000-141 Latest Study Materials of '\n",
      "             'dollars on things like ice packs and heat shields just to send '\n",
      "             'someone something, free boxes are awesome.\\n'\n",
      "             'The problem here is that puffer fish contain C1000-141 '\n",
      "             'Simulations Pdf tetrodotoxin, Next we have to tackle the '\n",
      "             'question of who should be included in the discussion, In the '\n",
      "             'scenarios we study, there is C1000-141 Reliable Practice '\n",
      "             'Questions usually some financial motive for Axel to extract or '\n",
      "             'alter information in the program.\\n'\n",
      "             'Pass Guaranteed 2022 IBM C1000-141 Authoritative Latest Study '\n",
      "             'Materials\\n'\n",
      "             'After the payment for our C1000-141 exam materials is '\n",
      "             'successful, you will receive an email from our system within '\n",
      "             '5-10 minutes; then, click on the link to log on and you can use '\n",
      "             'C1000-141 preparation materials to study immediately.\\n'\n",
      "             'The firewall allows Web traffic to the Web C1000-141 Latest '\n",
      "             'Study Materials server, and blocks all other incoming traffic, '\n",
      "             \"In addition, if you don't haveAdobe Creative Suite, you can only \"\n",
      "             'gain Study Materials PEGAPCSA87V1 Review access to the full '\n",
      "             'Version Cue feature set by participating in a shared project;\\n'\n",
      "             \"They discuss the benefits of sketchnoting, Mike's specific style \"\n",
      "             '350-601 Exams Dumps and techniques, and the rise of the '\n",
      "             'Sketchnote Army, Simple tactics for reducing this effect include '\n",
      "             'placing furniture or hanging acoustic-dampening material, such '\n",
      "             'as fabrics C1000-141 Latest Study Materials or oil paintings, on '\n",
      "             'or near the back wall to either absorb the sound or cause it to '\n",
      "             'reflect in a different direction.\\n'\n",
      "             'After a wireless client connects, the attacker attempts to trick '\n",
      "             'C1000-141 New Learning Materials the wireless client into giving '\n",
      "             'up valuable information, or else the attacker compromises the '\n",
      "             'client device in some way.\\n'\n",
      "             'They allow you to experiment quicker, This new '\n",
      "             'https://certkingdom.vce4dumps.com/C1000-141-latest-dumps.html '\n",
      "             'edition will be the new text of choice for anyone concerned with '\n",
      "             'system identification theory and practice, You may be anxious to '\n",
      "             'get your '\n",
      "             'https://prep4sure.examtorrent.com/C1000-141-exam-papers.html '\n",
      "             \"first clients, but don't fall into the trap of promising too \"\n",
      "             'much and delivering too little.\\n'\n",
      "             'Master IBM C1000-141 Exam Topics\\n'\n",
      "             'All our after-sale service staff is professional and patience so '\n",
      "             \"you don't need to have any worry anything about purchasing our \"\n",
      "             'IBM C1000-141 exam simulation: IBM Maximo Manage v8.x '\n",
      "             'Administrator.\\n'\n",
      "             'The C1000-141 pdf training guide can help you to figure out the '\n",
      "             'actual area where you are confused, As the unprecedented '\n",
      "             'intensity of talents comes in great numbers, what H19-336 '\n",
      "             'Upgrade Dumps abilities should a talent of modern time possess '\n",
      "             'and finally walk to the success?\\n'\n",
      "             'So with our C1000-141 exam questions, not only you can pass the '\n",
      "             'exam with ease with 100% pass guarantee, but also you can learn '\n",
      "             'the most professional and specilized knowledge in this field!\\n'\n",
      "             'Exam candidates hold great purchasing desire for our C1000-141 '\n",
      "             'study questions which contribute to successful experience of '\n",
      "             'former exam candidates with high quality and high efficiency.\\n'\n",
      "             'Our C1000-141 study materials are compiled and tested by our '\n",
      "             'expert, Besides, they made three versions for your reference, '\n",
      "             'the PDF, APP and Online software version.\\n'\n",
      "             'Our questions and answers include all the questions which may '\n",
      "             'appear in C1000-141 Latest Study Materials the exam and all the '\n",
      "             'approaches to answer the questions, It is obvious that the sales '\n",
      "             'volume of our study materials is increasing every year.\\n'\n",
      "             'However, it is an indisputable fact that a large number of '\n",
      "             'people fail to pass the C1000-141 examination each year, some of '\n",
      "             'them may choose to give it up while others may still choose to '\n",
      "             'insist.\\n'\n",
      "             'C1000-141 test prep can help you in a very short period of time '\n",
      "             'to prove yourself perfectly and efficiently, Apart from the '\n",
      "             'advantage of free renewal in one year, our exam prep offers you '\n",
      "             'constant discounts so that you can save a large amount of money '\n",
      "             'concerning buying our C1000-141 training materials.\\n'\n",
      "             'Being an exam candidate in this area, we believe after passing '\n",
      "             'the exam by the help of our C1000-141 practice materials, you '\n",
      "             'will only learn a lot from this C1000-141 exam but can handle '\n",
      "             'many problems emerging in a long run.\\n'\n",
      "             'Under the help of the real C1000-141 test simulation, you can '\n",
      "             'have a good command of key points which are more likely to be '\n",
      "             'tested in the real test, Our IBM Maximo Manage v8.x '\n",
      "             \"Administrator exams training pdf won't make you wait for such a \"\n",
      "             'long time.\\n'\n",
      "             'And we have free demos for you to download before you decide to '\n",
      "             'purchase.\\n'\n",
      "             'NEW QUESTION: 1\\n'\n",
      "             'A. Option B\\n'\n",
      "             'B. Option C\\n'\n",
      "             'C. Option D\\n'\n",
      "             'D. Option A\\n'\n",
      "             'In Group Policy Object Editor, click Computer Configuration, '\n",
      "             'click Windows Settings, click Security Settings, click Local '\n",
      "             'Policies, and then click Security Options. In the details pane, '\n",
      "             'double-click Accounts: Rename administrator account. The '\n",
      "             'Security Options node includes security settings regarding '\n",
      "             'interactive logon, digital signing of data, restrictions of '\n",
      "             'access to floppy and CD-ROM drives, unsigned driver '\n",
      "             'installations as well as logon dialog box behavior. This '\n",
      "             'category also includes options to configure authentication and '\n",
      "             'communication security within Active Directory.\\n'\n",
      "             'NEW QUESTION: 2\\n'\n",
      "             'NEW QUESTION: 3\\n'\n",
      "             'NEW QUESTION: 4\\n'\n",
      "             'A solution architect is migrating an existing workload to AWS '\n",
      "             'Fargate. The task can only run in a private subnet within the '\n",
      "             'VPC where there is no direct connectivity from outside the '\n",
      "             'system to the application. When the Fargate task is launched, '\n",
      "             'the task fails with the following error:\\n'\n",
      "             'How should the solution architect correct this error?\\n'\n",
      "             'A. Ensure the task Is set to DISABLED for the auto-assign public '\n",
      "             'IP setting when launching the task.\\n'\n",
      "             'Configure a NAT gateway in the private subnet in the VPC to '\n",
      "             'route requests to the internet\\n'\n",
      "             'B. Ensure the task is set to ENABLED for the auto-assign public '\n",
      "             'IP selling when launching the task.\\n'\n",
      "             'C. Ensure the network mode is set to bridge in the Fargate task '\n",
      "             'definition.\\n'\n",
      "             'D. Ensure the task is set to DISABLED Tor the auto-assign public '\n",
      "             'IP setting when launching the task.\\n'\n",
      "             'Configure a NAT gateway in the public subnet in the VPC to route '\n",
      "             'requests to the internet',\n",
      " 'response': 'To correct the error regarding the Fargate task failing in a '\n",
      "             'private subnet without direct connectivity to the internet, the '\n",
      "             'solution architect should:\\n'\n",
      "             '\\n'\n",
      "             '**D. Ensure the task is set to DISABLED for the auto-assign '\n",
      "             'public IP setting when launching the task. Configure a NAT '\n",
      "             'gateway in the public subnet in the VPC to route requests to the '\n",
      "             'internet.**\\n'\n",
      "             '\\n'\n",
      "             'This approach is correct because the task does not require a '\n",
      "             'public IP while it resides in a private subnet. By configuring a '\n",
      "             'NAT gateway in the public subnet, the Fargate task can access '\n",
      "             'the internet for any outbound requests without requiring direct '\n",
      "             'inbound access from the internet, thus maintaining security and '\n",
      "             \"compliance with the architecture's requirements.\"}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(myJsonlGz[460])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is just a single item, I think I'll just delete this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del myJsonlGz[460]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that that's done, let's do some...\n",
    "\n",
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in myJsonlGz:\n",
    "    # Regex thing for extracting ai step-by-step and reasoning evaluations or \"judgements\"\n",
    "    judgement = re.findall(r\":(.*)\", line[\"response\"])\n",
    "    \n",
    "    # Regex thing for removing any extra characters inserted by the LLM\n",
    "    cleanedJudgement = [\"\".join(re.findall(r\"[^ *]\", i)) for i in judgement]\n",
    "    \n",
    "    # cleanedJudgement[0] contains data on if the current document contains step-by-step data or not\n",
    "    if cleanedJudgement[0].lower() == \"yes\":\n",
    "        line[\"step-by-step\"] = 1\n",
    "    else:\n",
    "        line[\"step-by-step\"] = 0\n",
    "\n",
    "    # cleanedJudgement[1] contains data on if the current document contains reasoning data or not\n",
    "    if cleanedJudgement[1].lower() == \"yes\":\n",
    "        line[\"reasoning\"] = 1\n",
    "    else:\n",
    "        line[\"reasoning\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating train-test-split\n",
    "trainList, testList = train_test_split(myJsonlGz, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create a dataset from our data\n",
    "trainDs = Dataset.from_list(trainList)\n",
    "testDs = Dataset.from_list(testList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqI4QPHFddet"
   },
   "source": [
    "<h1> Step 2: Classifier training and evaluation </h1>\n",
    "\n",
    "\n",
    "*   Which target did you choose?\n",
    "*   Label distribution and majority baseline\n",
    "*   Classifier performance\n",
    "*   Manual inspection of the classifier output, what kinds of mistakes it makes?\n",
    "*   What is the composition of the data we gave you? What does it mean for your results?\n",
    "*   Concusions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenization ###\n",
    "\n",
    "# tokenizer import\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# tokenization function\n",
    "def tokenize(dataset):\n",
    "    \"\"\"\n",
    "    NOTE: distilbert-base-uncased has a processing limit of 512 tokens. Truncation cuts the rest off. This results \n",
    "    in data loss if the input document is longer than 512 tokens. In the data exploration phase I noted that some \n",
    "    documents are indeed longer than 512 words (1 token ‚âà 1 word) so this does matter here. Right now I'll just \n",
    "    keep on using distilbert and hope that the 512 token limit doesn't hurt the performance too much (‚âà<500 first \n",
    "    words should be enough to decide whether document has step-by-step instruction or not, but if performance is \n",
    "    obviously really bad, I might have to change the underlying model to something that can handle more tokens)\n",
    "    \"\"\"\n",
    "    return tokenizer(dataset[\"document\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "u6nkkhpxdvsb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90cf39db56740f9bab92df00e15895b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2a700a24434cb68f8c87b46a47a9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/503 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizing datasets\n",
    "trainTokenized = trainDs.map(tokenize, batched=True)\n",
    "testTokenized = testDs.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'response', 'step-by-step', 'reasoning', 'input_ids', 'attention_mask', 'label']\n"
     ]
    }
   ],
   "source": [
    "# Defining labels\n",
    "\"\"\"\n",
    "After reading through a ton of unclear documentation, random kaggle notebooks and whatever chatgpt hallucinated, I \n",
    "now know that doing superwised fine-tuning using the transformers library requires the dataset to have a \"labels\" \n",
    "column Which has to have integers. So basically I can't just say to the API, \"Hey my labels are in the \n",
    "\"step-by-step\"-column\", instead I have to explicitly define a \"label\" column in my dataset. Ain't that just \n",
    "wonderful! So here is a function that can duplicate a given column as \"label\" now. \n",
    "\n",
    "PS: Labels have to be integer and now they are boolean. Time to modify and rerun the data preparation code!\n",
    "\"\"\"\n",
    "LABELS_AT = \"step-by-step\"    # Change this if you want to train for the other prediction target\n",
    "trainTokenized = trainTokenized.add_column(\"label\", trainTokenized[LABELS_AT])\n",
    "testTokenized = testTokenized.add_column(\"label\", testTokenized[LABELS_AT])\n",
    "\n",
    "print(testTokenized.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the classification model. num_labels = 2 for binary classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training ###\n",
    "\n",
    "# Training arguments for the trainer\n",
    "training_args = TrainingArguments(\"out\",eval_strategy=\"epoch\",logging_strategy=\"no\",save_strategy=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up evaluation metrics for the trainer\n",
    "\n",
    "# This is copied from the Huggingface documentation for training (fine-tuning), although I may modify it later\n",
    "# URL: https://huggingface.co/docs/transformers/training\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # convert the logits to their predicted class\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer setup\n",
    "trainer = Trainer(model, training_args, train_dataset=trainTokenized, eval_dataset=testTokenized, compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='113' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [113/756 1:11:59 < 6:57:01, 0.03 it/s, Epoch 0.44/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23733/3558831785.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Actually training (fine-tuning) the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2172\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2529\u001b[0m                     \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2530\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m                     if (\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3674\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3675\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3677\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3730\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3731\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3732\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3733\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m         distilbert_output = self.distilbert(\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    795\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m         return self.transformer(\n\u001b[0m\u001b[1;32m    798\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    548\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    551\u001b[0m                     \u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                     \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;31m# Feed Forward Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0mffn_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tanulas/textual_data_analysis/tdavenv/lib/python3.12/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Actually training (fine-tuning) the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwaD73JQdlLl"
   },
   "source": [
    "<h1> Bonus step </h1>\n",
    "\n",
    "(leave empty if you do not do this)\n",
    "\n",
    "*   Prompt design\n",
    "*   Build (prompt,response pairs)\n",
    "*   Turn into HF Dataset and save\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCYHbUKRgvce"
   },
   "outputs": [],
   "source": [
    "#work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUUBswOEggP6"
   },
   "source": [
    "<h1> Summary and Conclusions </h1>\n",
    "\n",
    "* Brief TL;DR -style summary and main conclusions of your project."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOlRTC6KZD5KSelVJqSxXMx",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tdavenv",
   "language": "python",
   "name": "tdavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
