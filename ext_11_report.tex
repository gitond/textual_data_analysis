\documentclass{article}

% Import libraries
\usepackage{xcolor}	% for text colors
\usepackage{fancyhdr}	% for headers
\usepackage{hyperref}	% for hyperlinks
\usepackage{listings}	% for embedding code

% Document settings
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{lightgray},   
    commentstyle=\color{lime},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{darkgray},
    stringstyle=\color{purple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
\begin{document}
\pagestyle{fancy}
\fancyhead{}

% Header
\fancyhead[LO,LE]{Exercise task 11}
\fancyhead[CO,CE]{TKO\_8964\_3006}
\fancyhead[RO,RE]{by Botond Ortutay}

% Instructions
\textbf{Exercise task 11} \\\\\\
\textbf{Instructions:} \\\\\\
\textbf{Building a basic chatbot} \\\\
In this exercise, you will build a minimal chatbot using the transformers 
library and a pretrained model. \\\\
First, if you haven't done so already, study the 
\href{https://github.com/TurkuNLP/textual-data-analysis-course/blob/main/llms_using_transformers_library.ipynb}{Using LLMs with transformers}
notebook. That should give you all the necessary prerequisites for 
completing this exercise. \\\\
Using what you learned from that notebook, write a python script that takes 
input from a user, passes that to an LLM, and prints out the LLM response, 
repeating until the user requests to exit while 
\textbf{\underline{keeping track of the message history.}} 
You can use this template as a starting point \\\\
\begin{lstlisting}[language=Python]
[YOUR CODE HERE: load model]

messages = []
while True:
    user_input = input('Say something ("exit" to quit): ')
    if user_input == 'exit':
        break
    messages.append({'role': 'user', 'content': user_input})
    [YOUR CODE HERE:  call model, print its output, and add it to messages]
\end{lstlisting}

\\\\
(You may want to tweak generation parameters and add additional features 
like having the text "reset" reset the message queue to make testing easier.)
\\\\
You can use e.g. one of the following models:
\\
\begin{itemize}
	\item HuggingFaceTB/SmolLM2-135M-Instruct
	\item HuggingFaceTB/SmolLM2-360M-Instruct
	\item HuggingFaceTB/SmolLM2-1.7B-Instruct
\end{itemize}
\\
(For best results, use the largest model with GPU acceleration. If you're 
running on a GPU with more memory, you can of course use even larger 
models, just pick an appropriate one from https://huggingface.co/models.)
\\\\
Test your chatbot with the following and provide logs of your discussions:
\\
\begin{enumerate}
	\item At least 5 questions about basic facts about the world (e.g. the capital of a country)
	\item At least 5 arithmetic questions ranging from trivial ("what is 1+1?") to more complex
	\item Inform the system of a secret word (e.g. "zebra"), then after a few other questions ask it what the secret word is. Make sure you understand where the memory of that secret word is.
\end{enumerate}
\\
In very brief, what did you think about the capabilities of the chatbot you 
created? Of the factors impacting LLM performance that we discussed on the 
lecture, which one (or ones) do you think are primarily limiting its 
performance?
\\\\\\\\

% Solutions
\textbf{Solutions:} \\\\\\
The code is in a separate file (\texttt{exercise\_task\_11.py}). I used the 
medium model (\texttt{HuggingFaceTB/SmolLM2-360M-Instruct}) because the 
largest one decided to not run on my graphics card. Below are the logs of 
the conversation history as generated by my chatbot (blue, bold text: user; 
regular text: system):
\\\\\\\\

% Print logs
\textbf{Logs:} \\\\
\input{ext_11_logs.tex}
\\\\\\\\

% Question answers
\textbf{In very brief, what did you think about the capabilities of the chatbot you created?} \\\\
It's interesting. I wonder how much better would it have been with the 
better model. The message history definitely affects the outputs (see logs 
above). Sometimes the chatbot would generate no output or would answer in a 
way that made no sense, so it's not on chatGPT level quite yet. Sometimes 
I'd have manipulate the correct answers out of the bot when it wouldn't 
tell me. It was also easily "tricked" by math trick-questions. \\\\
\textbf{Of the factors impacting LLM performance that we discussed on the lecture, which one (or ones) do you think are primarily limiting its performance?} \\\\
Well it definitely is focused on English language first.

\end{document}
